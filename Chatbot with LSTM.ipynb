{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the Cornell Movie Dialog dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Utterances dataset has been uploaded to the /data folder.\n",
    "\n",
    "We do not need the metadata, so extract only utterance text and ID, conversation ID, and the \"reply-to\" field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "raw_data = []\n",
    "\n",
    "input_path = './data/utterances.jsonl'\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    \n",
    "    for line in f:\n",
    "        \n",
    "        line_data = json.loads(line.rstrip('\\n|\\r'))\n",
    "        \n",
    "        line_data_dict = {}\n",
    "        \n",
    "        line_data_dict['id'] = line_data['id']\n",
    "        line_data_dict['conversation_id'] = line_data['conversation_id']\n",
    "        line_data_dict['text'] = line_data['text']\n",
    "        line_data_dict['reply_to'] = line_data['reply-to']\n",
    "        \n",
    "        raw_data.append(line_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Pandas dataframe for ease of processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_df = pd.DataFrame(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>text</th>\n",
       "      <th>reply_to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1045</td>\n",
       "      <td>L1044</td>\n",
       "      <td>They do not!</td>\n",
       "      <td>L1044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "      <td>They do to!</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L985</td>\n",
       "      <td>L984</td>\n",
       "      <td>I hope so.</td>\n",
       "      <td>L984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "      <td>She okay?</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L925</td>\n",
       "      <td>L924</td>\n",
       "      <td>Let's go.</td>\n",
       "      <td>L924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>L924</td>\n",
       "      <td>L924</td>\n",
       "      <td>Wow</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>L872</td>\n",
       "      <td>L870</td>\n",
       "      <td>Okay -- you're gonna need to learn how to lie.</td>\n",
       "      <td>L871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>L871</td>\n",
       "      <td>L870</td>\n",
       "      <td>No</td>\n",
       "      <td>L870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "      <td>I'm kidding.  You know how sometimes you just ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>L869</td>\n",
       "      <td>L866</td>\n",
       "      <td>Like my fear of wearing pastels?</td>\n",
       "      <td>L868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id conversation_id                                               text  \\\n",
       "0  L1045           L1044                                       They do not!   \n",
       "1  L1044           L1044                                        They do to!   \n",
       "2   L985            L984                                         I hope so.   \n",
       "3   L984            L984                                          She okay?   \n",
       "4   L925            L924                                          Let's go.   \n",
       "5   L924            L924                                                Wow   \n",
       "6   L872            L870     Okay -- you're gonna need to learn how to lie.   \n",
       "7   L871            L870                                                 No   \n",
       "8   L870            L870  I'm kidding.  You know how sometimes you just ...   \n",
       "9   L869            L866                   Like my fear of wearing pastels?   \n",
       "\n",
       "  reply_to  \n",
       "0    L1044  \n",
       "1     None  \n",
       "2     L984  \n",
       "3     None  \n",
       "4     L924  \n",
       "5     None  \n",
       "6     L871  \n",
       "7     L870  \n",
       "8     None  \n",
       "9     L868  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a list of exchanges and convert to a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry will contain a \"call\" and a \"response\". For a given entry, we identify its \"call\" by using the \"reply_to\" field. This way, we get an appropriate response for each utterance (except the first one in each dialogue, for which the \"response_to\" field is NONE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_id_list = list(lines_df.conversation_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limiting the size of the list to 10,000 entries just to get the model working; it takes too long to process the entire list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_id_list = conversation_id_list[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 : processing conversation_id L4219\n",
      "1000 : processing conversation_id L8094\n"
     ]
    }
   ],
   "source": [
    "exchange_list = []\n",
    "index = 0\n",
    "\n",
    "for conversation_id in conversation_id_list:\n",
    "    \n",
    "    index += 1\n",
    "\n",
    "    temp_df = lines_df.loc[lines_df['conversation_id'] == conversation_id]\n",
    "    \n",
    "    utterance_list = temp_df.to_dict('records')\n",
    "    \n",
    "    utterance_dict = {}\n",
    "\n",
    "    for utterance in utterance_list:\n",
    "\n",
    "        temp_dict = {}\n",
    "        temp_dict['text'] = utterance['text']\n",
    "        temp_dict['reply_to'] = utterance['reply_to']\n",
    "        utterance_dict[utterance['id']] = temp_dict\n",
    "        \n",
    "    for utterance in utterance_dict.keys():\n",
    "    \n",
    "        call_id = utterance_dict[utterance]['reply_to']\n",
    "\n",
    "        if call_id != None:\n",
    "            \n",
    "            exchange_list.append({'CONVERSATION':conversation_id, 'EXCHANGE': call_id + '->' + utterance, 'CALL':utterance_dict[call_id]['text'], 'RESPONSE':utterance_dict[utterance]['text']})\n",
    "            \n",
    "    if index % 500 == 0:\n",
    "        print(index, ': processing conversation_id', conversation_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange_df = pd.DataFrame.from_dict(exchange_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONVERSATION</th>\n",
       "      <th>EXCHANGE</th>\n",
       "      <th>CALL</th>\n",
       "      <th>RESPONSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044-&gt;L1045</td>\n",
       "      <td>They do to!</td>\n",
       "      <td>They do not!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L984</td>\n",
       "      <td>L984-&gt;L985</td>\n",
       "      <td>She okay?</td>\n",
       "      <td>I hope so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L924</td>\n",
       "      <td>L924-&gt;L925</td>\n",
       "      <td>Wow</td>\n",
       "      <td>Let's go.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L870</td>\n",
       "      <td>L871-&gt;L872</td>\n",
       "      <td>No</td>\n",
       "      <td>Okay -- you're gonna need to learn how to lie.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L870</td>\n",
       "      <td>L870-&gt;L871</td>\n",
       "      <td>I'm kidding.  You know how sometimes you just ...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CONVERSATION      EXCHANGE  \\\n",
       "0        L1044  L1044->L1045   \n",
       "1         L984    L984->L985   \n",
       "2         L924    L924->L925   \n",
       "3         L870    L871->L872   \n",
       "4         L870    L870->L871   \n",
       "\n",
       "                                                CALL  \\\n",
       "0                                        They do to!   \n",
       "1                                          She okay?   \n",
       "2                                                Wow   \n",
       "3                                                 No   \n",
       "4  I'm kidding.  You know how sometimes you just ...   \n",
       "\n",
       "                                         RESPONSE  \n",
       "0                                    They do not!  \n",
       "1                                      I hope so.  \n",
       "2                                       Let's go.  \n",
       "3  Okay -- you're gonna need to learn how to lie.  \n",
       "4                                              No  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exchange_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now construct the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, preprocess text of call and response by converting to lower case and expanding contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \n",
    "    clean_text = text.lower()\n",
    "    \n",
    "    clean_text = re.sub('can\\'t', 'can not', clean_text)\n",
    "    clean_text = re.sub('won\\'t', 'will not', clean_text)\n",
    "    clean_text = re.sub('n\\'t', ' not', clean_text)\n",
    "    clean_text = re.sub('\\'ll', ' will', clean_text)\n",
    "    clean_text = re.sub('\\'m', ' am', clean_text)\n",
    "    clean_text = re.sub('he\\'s', 'he is', clean_text)\n",
    "    clean_text = re.sub('she\\'s', 'she is', clean_text)\n",
    "    clean_text = re.sub('it\\'s', 'it is', clean_text)\n",
    "    clean_text = re.sub('how\\'s', 'how is', clean_text)\n",
    "    clean_text = re.sub('that\\'s', 'that is', clean_text)\n",
    "    clean_text = re.sub('what\\'s', 'what is', clean_text)\n",
    "    clean_text = re.sub('here\\'s', 'here is', clean_text)\n",
    "    clean_text = re.sub('there\\'s', 'there is', clean_text)\n",
    "    clean_text = re.sub('let\\'s', 'let us', clean_text)\n",
    "    clean_text = re.sub('\\'re', ' are', clean_text)\n",
    "    clean_text = re.sub('\\'ve', ' have', clean_text)\n",
    "    clean_text = re.sub('\\'d', ' would', clean_text)\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange_df['CALL'] = exchange_df['CALL'].apply(preprocess_text)\n",
    "exchange_df['RESPONSE'] = exchange_df['RESPONSE'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONVERSATION</th>\n",
       "      <th>EXCHANGE</th>\n",
       "      <th>CALL</th>\n",
       "      <th>RESPONSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044-&gt;L1045</td>\n",
       "      <td>they do to!</td>\n",
       "      <td>they do not!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L984</td>\n",
       "      <td>L984-&gt;L985</td>\n",
       "      <td>she okay?</td>\n",
       "      <td>i hope so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L924</td>\n",
       "      <td>L924-&gt;L925</td>\n",
       "      <td>wow</td>\n",
       "      <td>let us go.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L870</td>\n",
       "      <td>L871-&gt;L872</td>\n",
       "      <td>no</td>\n",
       "      <td>okay -- you are gonna need to learn how to lie.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L870</td>\n",
       "      <td>L870-&gt;L871</td>\n",
       "      <td>i am kidding.  you know how sometimes you just...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CONVERSATION      EXCHANGE  \\\n",
       "0        L1044  L1044->L1045   \n",
       "1         L984    L984->L985   \n",
       "2         L924    L924->L925   \n",
       "3         L870    L871->L872   \n",
       "4         L870    L870->L871   \n",
       "\n",
       "                                                CALL  \\\n",
       "0                                        they do to!   \n",
       "1                                          she okay?   \n",
       "2                                                wow   \n",
       "3                                                 no   \n",
       "4  i am kidding.  you know how sometimes you just...   \n",
       "\n",
       "                                          RESPONSE  \n",
       "0                                     they do not!  \n",
       "1                                       i hope so.  \n",
       "2                                       let us go.  \n",
       "3  okay -- you are gonna need to learn how to lie.  \n",
       "4                                               no  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exchange_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_vocab(text, tokenizer, vocab):\n",
    "    \n",
    "    vocab.extend(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+\\'s|\\w+')\n",
    "raw_word_list = []\n",
    "\n",
    "exchange_df['CALL'].apply(add_to_vocab, tokenizer=tokenizer, vocab=raw_word_list)\n",
    "exchange_df['RESPONSE'].apply(add_to_vocab, tokenizer=tokenizer, vocab=raw_word_list)\n",
    "\n",
    "# Create a list of unique words sorted by frequency\n",
    "#\n",
    "word_counter = Counter(raw_word_list)\n",
    "\n",
    "unique_word_list = sorted(word_counter, key=word_counter.get, reverse=True)\n",
    "\n",
    "# Add tokens for start_of_sentence, end_of_sentence, padding\n",
    "#\n",
    "unique_word_list.insert(0, '<sos>')\n",
    "unique_word_list.insert(1, '<eos>')\n",
    "unique_word_list.insert(2, '<pad>')\n",
    "\n",
    "#  This is our vocab size\n",
    "#\n",
    "vocab_size = len(unique_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create mappings from words to IDs and back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings from words to IDs and from IDs to words\n",
    "#\n",
    "word_to_id = {word:id for id, word in enumerate(unique_word_list)}\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert calls and responses to IDs and pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine maximum call and response length, adding 2 for <sos> and <eos> tokens\n",
    "#\n",
    "# max_call_length = exchange_df.CALL.str.len().max() + 2\n",
    "# max_response_length = exchange_df.RESPONSE.str.len().max() + 2\n",
    "\n",
    "\n",
    "#  Set MAX_LENGTH = 128 for both call and response to reduce memory usage\n",
    "#\n",
    "max_call_length = 128\n",
    "max_response_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Function to convert a text to IDs and to pad to LENGTH with zeros\n",
    "#  If TEXT is longer than LENGTH, TEXT will be truncated\n",
    "#\n",
    "\n",
    "def text_to_ids(text, length):\n",
    "    \n",
    "    #  Initialize to all <pad> tokens\n",
    "    #\n",
    "    padded_seq = [2] * length\n",
    "    \n",
    "    padded_seq[0] = 0  #  index for <sos>\n",
    "    \n",
    "    word_list = tokenizer.tokenize(text)\n",
    "    \n",
    "    for index, word in enumerate(word_list):\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            padded_seq[index+1] = word_to_id[word]\n",
    "        \n",
    "        except KeyError:\n",
    "            \n",
    "            print('Key Error:', word)\n",
    "            \n",
    "        except IndexError:\n",
    "            \n",
    "            break\n",
    "            \n",
    "    eos_index = min(length-1, len(word_list)+1)\n",
    "    \n",
    "    padded_seq[eos_index] = 1  #  index for <eos>\n",
    "    \n",
    "    return padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert contexts and questions to IDs\n",
    "#\n",
    "exchange_df['call_to_ids'] = exchange_df.CALL.apply(text_to_ids, length=max_call_length)\n",
    "exchange_df['response_to_ids'] = exchange_df.RESPONSE.apply(text_to_ids, length=max_response_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONVERSATION</th>\n",
       "      <th>EXCHANGE</th>\n",
       "      <th>CALL</th>\n",
       "      <th>RESPONSE</th>\n",
       "      <th>call_to_ids</th>\n",
       "      <th>response_to_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044-&gt;L1045</td>\n",
       "      <td>they do to!</td>\n",
       "      <td>they do not!</td>\n",
       "      <td>[0, 37, 11, 6, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2...</td>\n",
       "      <td>[0, 37, 11, 7, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L984</td>\n",
       "      <td>L984-&gt;L985</td>\n",
       "      <td>she okay?</td>\n",
       "      <td>i hope so.</td>\n",
       "      <td>[0, 49, 90, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2...</td>\n",
       "      <td>[0, 4, 294, 51, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L924</td>\n",
       "      <td>L924-&gt;L925</td>\n",
       "      <td>wow</td>\n",
       "      <td>let us go.</td>\n",
       "      <td>[0, 2744, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 71, 93, 55, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L870</td>\n",
       "      <td>L871-&gt;L872</td>\n",
       "      <td>no</td>\n",
       "      <td>okay -- you are gonna need to learn how to lie.</td>\n",
       "      <td>[0, 38, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...</td>\n",
       "      <td>[0, 90, 3, 12, 87, 107, 6, 564, 47, 6, 593, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L870</td>\n",
       "      <td>L870-&gt;L871</td>\n",
       "      <td>i am kidding.  you know how sometimes you just...</td>\n",
       "      <td>no</td>\n",
       "      <td>[0, 4, 19, 633, 3, 25, 47, 374, 3, 44, 1313, 2...</td>\n",
       "      <td>[0, 38, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CONVERSATION      EXCHANGE  \\\n",
       "0        L1044  L1044->L1045   \n",
       "1         L984    L984->L985   \n",
       "2         L924    L924->L925   \n",
       "3         L870    L871->L872   \n",
       "4         L870    L870->L871   \n",
       "\n",
       "                                                CALL  \\\n",
       "0                                        they do to!   \n",
       "1                                          she okay?   \n",
       "2                                                wow   \n",
       "3                                                 no   \n",
       "4  i am kidding.  you know how sometimes you just...   \n",
       "\n",
       "                                          RESPONSE  \\\n",
       "0                                     they do not!   \n",
       "1                                       i hope so.   \n",
       "2                                       let us go.   \n",
       "3  okay -- you are gonna need to learn how to lie.   \n",
       "4                                               no   \n",
       "\n",
       "                                         call_to_ids  \\\n",
       "0  [0, 37, 11, 6, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2...   \n",
       "1  [0, 49, 90, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2...   \n",
       "2  [0, 2744, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "3  [0, 38, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...   \n",
       "4  [0, 4, 19, 633, 3, 25, 47, 374, 3, 44, 1313, 2...   \n",
       "\n",
       "                                     response_to_ids  \n",
       "0  [0, 37, 11, 7, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2...  \n",
       "1  [0, 4, 294, 51, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...  \n",
       "2  [0, 71, 93, 55, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...  \n",
       "3  [0, 90, 3, 12, 87, 107, 6, 564, 47, 6, 593, 1,...  \n",
       "4  [0, 38, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exchange_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Encoder, Decoder, and Seq2Seq model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use a single-layer LSTM for both the Encoder and the Decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "id": "oQLTP2Wmi1eB"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        \n",
    "        '''\n",
    "        vocab_size:     the size of the vocabulary\n",
    "        embedding_size: the size of the embedding\n",
    "        hidden_size:    the number of hidden state features\n",
    "\n",
    "        '''\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # self.embedding provides a vector representation of the inputs to our model\n",
    "        \n",
    "        # self.lstm, accepts the vectorized input and passes a hidden state\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # define the embedding layer based on the embedding_size parameter\n",
    "        #\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        \n",
    "        # initialize the hidden state to all zeros\n",
    "        #\n",
    "        self.hidden = torch.zeros(1, 1, hidden_size)\n",
    "        \n",
    "        # define the lstm layer based on the embedding dimension and hidden state size\n",
    "        #\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, 1) \n",
    "        \n",
    "    \n",
    "    def forward(self, source_batch):\n",
    "        \n",
    "        '''\n",
    "        Input:   source_batch, the source vector of the size INPUT_SEQ_LENGTH x BATCH_SIZE\n",
    "        Outputs: output, the encoder outputs\n",
    "                 hidden, the hidden state\n",
    "                 cell, the cell state\n",
    "        '''\n",
    "        #  Generate the embedding\n",
    "        #\n",
    "        embedding = self.embedding(source_batch.type(torch.LongTensor))\n",
    "               \n",
    "        #  Generate outputs. We care only about the hidden states and the cell states\n",
    "        #\n",
    "        _, hidden, cell = self.lstm(embedding)\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "      \n",
    "    def __init__(self, embedding_size, hidden_size, vocab_size):\n",
    "        \n",
    "        '''\n",
    "        vocab_size:     the size of the vocabulary\n",
    "        embedding_size: the size of the embedding\n",
    "        hidden_size:    the number of hidden state features\n",
    "\n",
    "        '''\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # self.embedding provides a vector representation of the target to our model\n",
    "        #\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        \n",
    "        # self.lstm, accepts the embeddings and outputs a hidden state\n",
    "        #\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, 1)\n",
    "\n",
    "        # self.ouput, a vector of the size 1 x vocab_size with probabilities for each word in vocab\n",
    "        #\n",
    "        self.output = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, source, hidden, cell):\n",
    "        \n",
    "        '''\n",
    "        Inputs:  source, the target vector\n",
    "                 hidden, the previous hidden state\n",
    "                 cell, the previous cell state\n",
    "                \n",
    "        Outputs: output, the prediction\n",
    "                 hidden, the new hidden state\n",
    "                 cell, the new cell state\n",
    "        '''\n",
    "        source = source.unsqueeze(0)\n",
    "        \n",
    "        # generate the embedding\n",
    "        #\n",
    "        embedding = self.embedding(source.type(torch.LongTensor))\n",
    "        \n",
    "        output, (hidden, cell) = self.lstm(source.type(torch.LongTensor), (hidden, cell))\n",
    "        \n",
    "        output = self.output(output.squeeze(0))\n",
    "        \n",
    "        return output, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, tf_ratio = 0.5):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.tf_ratio = tf_ratio\n",
    "    \n",
    "    \n",
    "    def forward(self, source_batch, target_batch):\n",
    "        \n",
    "        batch_size = source_batch.shape[1]\n",
    "\n",
    "        output_sequence = torch.zeros(max_response_length, batch_size, vocab_size)\n",
    "\n",
    "        hidden, cell = self.encoder(source_batch.type(torch.LongTensor))\n",
    "\n",
    "        predicted_word = target[0]\n",
    "\n",
    "        for index in range(1, max_response_length):\n",
    "\n",
    "            word_probabilities, hidden, cell = self.decoder(predicted_word, hidden, cell)\n",
    "\n",
    "            output_sequence[index] = word_probabilities\n",
    "\n",
    "            if random.random() < tf_ratio:\n",
    "            \n",
    "                predicted_word = target[index]\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                predicted_word = word_probabilities.argmax(1)\n",
    "\n",
    "        return output_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define hyperparameters and set up the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_ratio = 0.5\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "embedding_size = 300\n",
    "\n",
    "hidden_size = 1024\n",
    "\n",
    "encoder = Encoder(vocab_size, embedding_size, hidden_size).to(device)\n",
    "\n",
    "decoder = Decoder(embedding_size, hidden_size, vocab_size).to(device)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, tf_ratio).to(device)\n",
    "\n",
    "#  Ignore the padding token, which has index 2 in our vocab\n",
    "#\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=2).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(4164, 300)\n",
       "    (lstm): LSTM(300, 1024)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(4164, 300)\n",
       "    (lstm): LSTM(300, 1024)\n",
       "    (output): Linear(in_features=1024, out_features=4164, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for _, data in enumerate(dataloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        calls, responses = data\n",
    "        \n",
    "        outputs = model(calls, responses)\n",
    "        \n",
    "        # 1. as mentioned in the seq2seq section, we will\n",
    "        # cut off the first element when performing the evaluation\n",
    "        # 2. the loss function only works on 2d inputs\n",
    "        # with 1d targets we need to flatten each of them\n",
    "        \n",
    "        outputs_flatten = outputs[1:].view(-1, outputs.shape[-1])\n",
    "        responses_flatten = responses[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(outputs_flatten, responses_flatten)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for _, data in enumerate(dataloader):\n",
    "            \n",
    "            calls, responses = data\n",
    "            \n",
    "            # turn off teacher forcing\n",
    "            #\n",
    "            outputs = model(calls, responses, tf_ratio=0) \n",
    "            \n",
    "            outputs_flatten = outputs[1:].view(-1, outputs.shape[-1])\n",
    "            responses_flatten = responses[1:].view(-1)\n",
    "            \n",
    "            loss = criterion(outputs_flatten, responses_flatten)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training, validation, and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Split the dataframe\n",
    "#\n",
    "\n",
    "#  Use 20% of the entire dataframe for testing\n",
    "#\n",
    "test_df = exchange_df.sample(frac=0.2)\n",
    "\n",
    "#  Use 80% for training\n",
    "#\n",
    "train_df = exchange_df.drop(test_df.index)\n",
    "\n",
    "#  Use 20% of the training df for validation\n",
    "#\n",
    "validation_df = train_df.sample(frac=0.2)\n",
    "\n",
    "train_df = train_df.drop(validation_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Dataset class\n",
    "#\n",
    "class ExchangeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        \n",
    "        x = df.iloc[:,4].values.tolist()\n",
    "        y = df.iloc[:,5].values.tolist()\n",
    "        \n",
    "        self.x = torch.tensor(x, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ExchangeDataset(train_df)\n",
    "\n",
    "validation_dataset = ExchangeDataset(validation_df)\n",
    "\n",
    "test_dataset = ExchangeDataset(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-223-7b116e5a0745>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-215-1185f9a387af>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mcalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# 1. as mentioned in the seq2seq section, we will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-211-fd2e0f4e1e38>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source_batch, target_batch)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moutput_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_response_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mpredicted_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-209-453cc58e2039>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source_batch)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m#  Generate the embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m#  Generate outputs. We care only about the hidden states and the cell states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2181\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2183\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion)\n",
    "    valid_loss = evaluate(model, validation_dataloader, criterion)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
