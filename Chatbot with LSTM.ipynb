{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the Cornell Movie Dialog dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Utterances dataset has been uploaded to the /data folder.\n",
    "\n",
    "We do not need the metadata, so extract only utterance text and ID, conversation ID, and the \"reply-to\" field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand contractions, tokenize utterance text and trim to MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+\\'s|\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_token_list(text):\n",
    "    \n",
    "    clean_text = text.lower()\n",
    "    \n",
    "    clean_text = re.sub('can\\'t', 'can not', clean_text)\n",
    "    clean_text = re.sub('won\\'t', 'will not', clean_text)\n",
    "    clean_text = re.sub('n\\'t', ' not', clean_text)\n",
    "    clean_text = re.sub('\\'ll', ' will', clean_text)\n",
    "    clean_text = re.sub('\\'m', ' am', clean_text)\n",
    "    clean_text = re.sub('he\\'s', 'he is', clean_text)\n",
    "    clean_text = re.sub('she\\'s', 'she is', clean_text)\n",
    "    clean_text = re.sub('it\\'s', 'it is', clean_text)\n",
    "    clean_text = re.sub('how\\'s', 'how is', clean_text)\n",
    "    clean_text = re.sub('that\\'s', 'that is', clean_text)\n",
    "    clean_text = re.sub('what\\'s', 'what is', clean_text)\n",
    "    clean_text = re.sub('here\\'s', 'here is', clean_text)\n",
    "    clean_text = re.sub('there\\'s', 'there is', clean_text)\n",
    "    clean_text = re.sub('let\\'s', 'let us', clean_text)\n",
    "    clean_text = re.sub('\\'re', ' are', clean_text)\n",
    "    clean_text = re.sub('\\'ve', ' have', clean_text)\n",
    "    clean_text = re.sub('\\'d', ' would', clean_text)\n",
    "    \n",
    "    return tokenizer.tokenize(clean_text)[:max_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get utterances from file and apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = []\n",
    "\n",
    "input_path = './data/utterances.jsonl'\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    \n",
    "    for line in f:\n",
    "        \n",
    "        line_data = json.loads(line.rstrip('\\n|\\r'))\n",
    "        \n",
    "        line_data_dict = {}\n",
    "        \n",
    "        line_data_dict['id'] = line_data['id']\n",
    "        line_data_dict['conversation_id'] = line_data['conversation_id']\n",
    "        line_data_dict['token_list'] = clean_token_list(line_data['text'])\n",
    "        line_data_dict['reply_to'] = line_data['reply-to']\n",
    "        \n",
    "        raw_data.append(line_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Pandas dataframe for ease of processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_df = pd.DataFrame(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>token_list</th>\n",
       "      <th>reply_to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1045</td>\n",
       "      <td>L1044</td>\n",
       "      <td>[they, do, not]</td>\n",
       "      <td>L1044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "      <td>[they, do, to]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L985</td>\n",
       "      <td>L984</td>\n",
       "      <td>[i, hope, so]</td>\n",
       "      <td>L984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "      <td>[she, okay]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L925</td>\n",
       "      <td>L924</td>\n",
       "      <td>[let, us, go]</td>\n",
       "      <td>L924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>L924</td>\n",
       "      <td>L924</td>\n",
       "      <td>[wow]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>L872</td>\n",
       "      <td>L870</td>\n",
       "      <td>[okay, you, are, gonna, need, to, learn, how, ...</td>\n",
       "      <td>L871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>L871</td>\n",
       "      <td>L870</td>\n",
       "      <td>[no]</td>\n",
       "      <td>L870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "      <td>[i, am, kidding, you, know, how, sometimes, yo...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>L869</td>\n",
       "      <td>L866</td>\n",
       "      <td>[like, my, fear, of, wearing, pastels]</td>\n",
       "      <td>L868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id conversation_id                                         token_list  \\\n",
       "0  L1045           L1044                                    [they, do, not]   \n",
       "1  L1044           L1044                                     [they, do, to]   \n",
       "2   L985            L984                                      [i, hope, so]   \n",
       "3   L984            L984                                        [she, okay]   \n",
       "4   L925            L924                                      [let, us, go]   \n",
       "5   L924            L924                                              [wow]   \n",
       "6   L872            L870  [okay, you, are, gonna, need, to, learn, how, ...   \n",
       "7   L871            L870                                               [no]   \n",
       "8   L870            L870  [i, am, kidding, you, know, how, sometimes, yo...   \n",
       "9   L869            L866             [like, my, fear, of, wearing, pastels]   \n",
       "\n",
       "  reply_to  \n",
       "0    L1044  \n",
       "1     None  \n",
       "2     L984  \n",
       "3     None  \n",
       "4     L924  \n",
       "5     None  \n",
       "6     L871  \n",
       "7     L870  \n",
       "8     None  \n",
       "9     L868  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a list of exchanges and convert to a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry will contain a \"call\" and a \"response\". For a given entry, we identify its \"call\" by using the \"reply_to\" field. This way, we get an appropriate response for each utterance (except the first one in each dialogue, for which the \"response_to\" field is NONE.\n",
    "\n",
    "I will limit the size of the list to 1,000 conversations for demonstration purposes, as the full list takes too long to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_id_list = list(lines_df.conversation_id.unique())[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesed 500 conversations\n",
      "Procesed 1000 conversations\n"
     ]
    }
   ],
   "source": [
    "exchange_list = []\n",
    "index = 0\n",
    "\n",
    "for conversation_id in conversation_id_list:\n",
    "    \n",
    "    index += 1\n",
    "\n",
    "    temp_df = lines_df.loc[lines_df['conversation_id'] == conversation_id]\n",
    "    \n",
    "    utterance_list = temp_df.to_dict('records')\n",
    "    \n",
    "    utterance_dict = {}\n",
    "\n",
    "    for utterance in utterance_list:\n",
    "\n",
    "        temp_dict = {}\n",
    "        temp_dict['token_list'] = utterance['token_list']\n",
    "        temp_dict['reply_to'] = utterance['reply_to']\n",
    "        utterance_dict[utterance['id']] = temp_dict\n",
    "        \n",
    "    for utterance in utterance_dict.keys():\n",
    "    \n",
    "        call_id = utterance_dict[utterance]['reply_to']\n",
    "\n",
    "        if call_id != None:\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                exchange_list.append({'CONVERSATION':conversation_id, 'EXCHANGE': call_id + '->' + utterance, 'CALL':utterance_dict[call_id]['token_list'], 'RESPONSE':utterance_dict[utterance]['token_list']})\n",
    "            \n",
    "            except KeyError:\n",
    "                \n",
    "                pass\n",
    "            \n",
    "    if index % 500 == 0:\n",
    "        \n",
    "        print('Procesed', index, 'conversations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange_df = pd.DataFrame.from_dict(exchange_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONVERSATION</th>\n",
       "      <th>EXCHANGE</th>\n",
       "      <th>CALL</th>\n",
       "      <th>RESPONSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044-&gt;L1045</td>\n",
       "      <td>[they, do, to]</td>\n",
       "      <td>[they, do, not]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L984</td>\n",
       "      <td>L984-&gt;L985</td>\n",
       "      <td>[she, okay]</td>\n",
       "      <td>[i, hope, so]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L924</td>\n",
       "      <td>L924-&gt;L925</td>\n",
       "      <td>[wow]</td>\n",
       "      <td>[let, us, go]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L870</td>\n",
       "      <td>L871-&gt;L872</td>\n",
       "      <td>[no]</td>\n",
       "      <td>[okay, you, are, gonna, need, to, learn, how, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L870</td>\n",
       "      <td>L870-&gt;L871</td>\n",
       "      <td>[i, am, kidding, you, know, how, sometimes, yo...</td>\n",
       "      <td>[no]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CONVERSATION      EXCHANGE  \\\n",
       "0        L1044  L1044->L1045   \n",
       "1         L984    L984->L985   \n",
       "2         L924    L924->L925   \n",
       "3         L870    L871->L872   \n",
       "4         L870    L870->L871   \n",
       "\n",
       "                                                CALL  \\\n",
       "0                                     [they, do, to]   \n",
       "1                                        [she, okay]   \n",
       "2                                              [wow]   \n",
       "3                                               [no]   \n",
       "4  [i, am, kidding, you, know, how, sometimes, yo...   \n",
       "\n",
       "                                            RESPONSE  \n",
       "0                                    [they, do, not]  \n",
       "1                                      [i, hope, so]  \n",
       "2                                      [let, us, go]  \n",
       "3  [okay, you, are, gonna, need, to, learn, how, ...  \n",
       "4                                               [no]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exchange_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle the dataframe, so we do not have to rerun every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"exchange_df.bin\", \"wb\") as f:\n",
    "    \n",
    "    pickle.dump(exchange_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now construct the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load exchanges dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"exchange_df.bin\", \"rb\") as f:\n",
    "    \n",
    "    exchange_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_vocab(word_list, vocab):\n",
    "    \n",
    "    vocab.extend(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW WORD LIST LENGTH = 47624\n"
     ]
    }
   ],
   "source": [
    "raw_word_list = []\n",
    "\n",
    "exchange_df['CALL'].apply(add_to_vocab, vocab=raw_word_list)\n",
    "exchange_df['RESPONSE'].apply(add_to_vocab, vocab=raw_word_list)\n",
    "\n",
    "print('RAW WORD LIST LENGTH =', len(raw_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE = 3778\n"
     ]
    }
   ],
   "source": [
    "word_counter = Counter(raw_word_list)\n",
    "\n",
    "unique_word_list = sorted(word_counter, key=word_counter.get, reverse=True)\n",
    "\n",
    "# Add tokens for start_of_sentence, end_of_sentence, padding\n",
    "#\n",
    "unique_word_list.insert(0, '<sos>')\n",
    "unique_word_list.insert(1, '<eos>')\n",
    "unique_word_list.insert(2, '<pad>')\n",
    "\n",
    "#  This is our vocabulary size\n",
    "#\n",
    "vocab_size = len(unique_word_list)\n",
    "\n",
    "print('VOCAB SIZE =', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create mappings from words to IDs and back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings from words to IDs and from IDs to words\n",
    "#\n",
    "word_to_id = {word:id for id, word in enumerate(unique_word_list)}\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert calls and responses to IDs and pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Function to convert a text to IDs and to pad to LENGTH with zeros\n",
    "#  If TEXT is longer than LENGTH, TEXT will be truncated\n",
    "#\n",
    "def text_to_ids(word_list):\n",
    "    \n",
    "    #  Initialize to all <pad> tokens\n",
    "    #\n",
    "    padded_seq = [2] * max_length\n",
    "    \n",
    "    padded_seq[0] = 0  #  index for <sos>\n",
    "    \n",
    "    for index, word in enumerate(word_list):\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            padded_seq[index+1] = word_to_id[word]\n",
    "        \n",
    "        except KeyError:\n",
    "            \n",
    "            print('Key Error:', word)\n",
    "            \n",
    "        except IndexError:\n",
    "            \n",
    "            break\n",
    "            \n",
    "    eos_index = min(max_length-1, len(word_list)+1)\n",
    "    \n",
    "    padded_seq[eos_index] = 1  #  index for <eos>\n",
    "    \n",
    "    return padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert contexts and questions to IDs\n",
    "#\n",
    "exchange_df['call_to_ids'] = exchange_df.CALL.apply(text_to_ids)\n",
    "exchange_df['response_to_ids'] = exchange_df.RESPONSE.apply(text_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONVERSATION</th>\n",
       "      <th>EXCHANGE</th>\n",
       "      <th>CALL</th>\n",
       "      <th>RESPONSE</th>\n",
       "      <th>call_to_ids</th>\n",
       "      <th>response_to_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044-&gt;L1045</td>\n",
       "      <td>[they, do, to]</td>\n",
       "      <td>[they, do, not]</td>\n",
       "      <td>[0, 42, 11, 6, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2...</td>\n",
       "      <td>[0, 42, 11, 7, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L984</td>\n",
       "      <td>L984-&gt;L985</td>\n",
       "      <td>[she, okay]</td>\n",
       "      <td>[i, hope, so]</td>\n",
       "      <td>[0, 51, 78, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2...</td>\n",
       "      <td>[0, 4, 271, 48, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L924</td>\n",
       "      <td>L924-&gt;L925</td>\n",
       "      <td>[wow]</td>\n",
       "      <td>[let, us, go]</td>\n",
       "      <td>[0, 2533, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 70, 84, 56, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L870</td>\n",
       "      <td>L871-&gt;L872</td>\n",
       "      <td>[no]</td>\n",
       "      <td>[okay, you, are, gonna, need, to, learn, how, ...</td>\n",
       "      <td>[0, 36, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...</td>\n",
       "      <td>[0, 78, 3, 12, 88, 102, 6, 507, 44, 6, 535, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L870</td>\n",
       "      <td>L870-&gt;L871</td>\n",
       "      <td>[i, am, kidding, you, know, how, sometimes, yo...</td>\n",
       "      <td>[no]</td>\n",
       "      <td>[0, 4, 19, 574, 3, 25, 44, 455, 3, 43, 1189, 2...</td>\n",
       "      <td>[0, 36, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CONVERSATION      EXCHANGE  \\\n",
       "0        L1044  L1044->L1045   \n",
       "1         L984    L984->L985   \n",
       "2         L924    L924->L925   \n",
       "3         L870    L871->L872   \n",
       "4         L870    L870->L871   \n",
       "\n",
       "                                                CALL  \\\n",
       "0                                     [they, do, to]   \n",
       "1                                        [she, okay]   \n",
       "2                                              [wow]   \n",
       "3                                               [no]   \n",
       "4  [i, am, kidding, you, know, how, sometimes, yo...   \n",
       "\n",
       "                                            RESPONSE  \\\n",
       "0                                    [they, do, not]   \n",
       "1                                      [i, hope, so]   \n",
       "2                                      [let, us, go]   \n",
       "3  [okay, you, are, gonna, need, to, learn, how, ...   \n",
       "4                                               [no]   \n",
       "\n",
       "                                         call_to_ids  \\\n",
       "0  [0, 42, 11, 6, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2...   \n",
       "1  [0, 51, 78, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2...   \n",
       "2  [0, 2533, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "3  [0, 36, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...   \n",
       "4  [0, 4, 19, 574, 3, 25, 44, 455, 3, 43, 1189, 2...   \n",
       "\n",
       "                                     response_to_ids  \n",
       "0  [0, 42, 11, 7, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2...  \n",
       "1  [0, 4, 271, 48, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...  \n",
       "2  [0, 70, 84, 56, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...  \n",
       "3  [0, 78, 3, 12, 88, 102, 6, 507, 44, 6, 535, 1,...  \n",
       "4  [0, 36, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exchange_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Encoder, Decoder, and Seq2Seq model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use a single-layer LSTM for both the Encoder and the Decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "        - source batch\n",
    "    Layer : \n",
    "        source batch -> Embedding -> LSTM\n",
    "    Output :\n",
    "        - LSTM hidden state\n",
    "        - LSTM cell state\n",
    "\n",
    "    Parmeters\n",
    "    ---------\n",
    "    vocab_size : int\n",
    "        Input dimension, should equal to the source vocab size.\n",
    "    \n",
    "    embedding_size : int\n",
    "        Embedding layer's dimension.\n",
    "        \n",
    "    hidden_size : int\n",
    "        LSTM Hidden/Cell state's dimension.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, embedding_size: int, hidden_size: int):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, 1)\n",
    "\n",
    "    def forward(self, source_batch: torch.LongTensor):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        source_batch : 2d torch.LongTensor\n",
    "            Batched tokenized source sentence of shape [sent len, batch size].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        hidden, cell : 3d torch.LongTensor\n",
    "            Hidden and cell state of the LSTM layer. Each state's shape\n",
    "            [n layers * n directions, batch size, hidden dim]\n",
    "        \"\"\"\n",
    "        embedding = self.embedding(source_batch) # [sent len, batch size, emb dim]\n",
    "        outputs, (hidden, cell) = self.lstm(embedding)\n",
    "        # outputs -> [sent len, batch size, hidden dim * n directions]\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "        - first token in the target batch\n",
    "        - LSTM hidden state from the encoder\n",
    "        - LSTM cell state from the encoder\n",
    "    Layer :\n",
    "        target batch -> Embedding -- \n",
    "                                   |\n",
    "        encoder hidden state ------|--> LSTM -> Linear\n",
    "                                   |\n",
    "        encoder cell state   -------\n",
    "        \n",
    "    Output :\n",
    "        - prediction\n",
    "        - LSTM hidden state\n",
    "        - LSTM cell state\n",
    "\n",
    "    Parmeters\n",
    "    ---------\n",
    "    output : int\n",
    "        Output dimension, should equal to the target vocab size.\n",
    "    \n",
    "    embedding_size : int\n",
    "        Embedding layer's dimension.\n",
    "        \n",
    "    hidden_size : int\n",
    "        LSTM Hidden/Cell state's dimension.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, embedding_size: int, hidden_size: int):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, 1)\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, source: torch.LongTensor, hidden: torch.FloatTensor, cell: torch.FloatTensor):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        source : 1d torch.LongTensor\n",
    "            Batched tokenized source sentence of shape [batch size].\n",
    "            \n",
    "        hidden, cell : 3d torch.FloatTensor\n",
    "            Hidden and cell state of the LSTM layer. Each state's shape\n",
    "            [n layers * n directions, batch size, hidden dim]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        prediction : 2d torch.LongTensor\n",
    "            For each token in the batch, the predicted target vobulary.\n",
    "            Shape [batch size, output dim]\n",
    "\n",
    "        hidden, cell : 3d torch.FloatTensor\n",
    "            Hidden and cell state of the LSTM layer. Each state's shape\n",
    "            [n layers * n directions, batch size, hidden dim]\n",
    "        \"\"\"\n",
    "        # [1, batch size, emb dim], the 1 serves as sent len\n",
    "        \n",
    "        embedding = self.embedding(source.unsqueeze(0))\n",
    "        outputs, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
    "        prediction = self.out(outputs.squeeze(0))\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source_batch: torch.LongTensor, target_batch: torch.LongTensor,\n",
    "                tf_ratio: float=0.5):\n",
    "\n",
    "        max_len, batch_size = target_batch.shape\n",
    "        target_vocab_size = self.decoder.vocab_size\n",
    "\n",
    "        # tensor to store decoder's output\n",
    "        outputs = torch.zeros(max_len, batch_size, target_vocab_size).to(device)\n",
    "\n",
    "        # last hidden & cell state of the encoder is used as the decoder's initial hidden state\n",
    "        hidden, cell = self.encoder(source_batch)\n",
    "\n",
    "        target = target_batch[0]\n",
    "        \n",
    "        for i in range(1, max_len):\n",
    "            \n",
    "            prediction, hidden, cell = self.decoder(target, hidden, cell)\n",
    "            outputs[i] = prediction\n",
    "\n",
    "            if random.random() < tf_ratio:\n",
    "                target = target_batch[i]\n",
    "            else:\n",
    "                target = prediction.argmax(1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define hyperparameters and set up the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_ratio = 0.5\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "embedding_size = 300\n",
    "\n",
    "hidden_size = 1024\n",
    "\n",
    "encoder = Encoder(vocab_size, embedding_size, hidden_size).to(device)\n",
    "\n",
    "decoder = Decoder(vocab_size, embedding_size, hidden_size).to(device)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "#  Ignore the padding token, which has index 2 in our vocab\n",
    "#\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=2)   #.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(3778, 300)\n",
       "    (lstm): LSTM(300, 1024)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(3778, 300)\n",
       "    (lstm): LSTM(300, 1024)\n",
       "    (out): Linear(in_features=1024, out_features=3778, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for _, data in enumerate(dataloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        calls, responses = data\n",
    "        \n",
    "        calls, responses = calls.type(torch.LongTensor), responses.type(torch.LongTensor)\n",
    "        \n",
    "        outputs = model(calls, responses)\n",
    "        \n",
    "        # 1. as mentioned in the seq2seq section, we will\n",
    "        # cut off the first element when performing the evaluation\n",
    "        # 2. the loss function only works on 2d inputs\n",
    "        # with 1d targets we need to flatten each of them\n",
    "        \n",
    "        outputs_flatten = outputs[1:].view(-1, outputs.shape[-1])\n",
    "        responses_flatten = responses[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(outputs_flatten, responses_flatten)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for _, data in enumerate(dataloader):\n",
    "            \n",
    "            calls, responses = data\n",
    "            \n",
    "            calls, responses = calls.type(torch.LongTensor), responses.type(torch.LongTensor)\n",
    "            \n",
    "            # turn off teacher forcing\n",
    "            #\n",
    "            outputs = model(calls, responses, tf_ratio=0) \n",
    "            \n",
    "            outputs_flatten = outputs[1:].view(-1, outputs.shape[-1])\n",
    "            responses_flatten = responses[1:].view(-1)\n",
    "            \n",
    "            loss = criterion(outputs_flatten, responses_flatten)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training, validation, and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Split the dataframe\n",
    "#\n",
    "\n",
    "#  Use 20% of the entire dataframe for testing\n",
    "#\n",
    "test_df = exchange_df.sample(frac=0.2)\n",
    "\n",
    "#  Use 80% for training\n",
    "#\n",
    "train_df = exchange_df.drop(test_df.index)\n",
    "\n",
    "#  Use 20% of the training df for validation\n",
    "#\n",
    "validation_df = train_df.sample(frac=0.2)\n",
    "\n",
    "train_df = train_df.drop(validation_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Dataset class\n",
    "#\n",
    "class ExchangeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        \n",
    "        x = df.iloc[:,4].values.tolist()\n",
    "        y = df.iloc[:,5].values.tolist()\n",
    "        \n",
    "        self.x = torch.tensor(x, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ExchangeDataset(train_df)\n",
    "\n",
    "validation_dataset = ExchangeDataset(validation_df)\n",
    "\n",
    "test_dataset = ExchangeDataset(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 6m 27s\n",
      "\tTrain Loss: 5.725 | Train PPL: 306.408\n",
      "\t Val. Loss: 5.505 |  Val. PPL: 245.980\n",
      "Epoch: 02 | Time: 6m 7s\n",
      "\tTrain Loss: 5.372 | Train PPL: 215.237\n",
      "\t Val. Loss: 5.671 |  Val. PPL: 290.393\n",
      "Epoch: 03 | Time: 7m 7s\n",
      "\tTrain Loss: 5.293 | Train PPL: 198.993\n",
      "\t Val. Loss: 5.850 |  Val. PPL: 347.141\n",
      "Epoch: 04 | Time: 8m 0s\n",
      "\tTrain Loss: 5.198 | Train PPL: 180.820\n",
      "\t Val. Loss: 5.908 |  Val. PPL: 367.940\n",
      "Epoch: 05 | Time: 9m 2s\n",
      "\tTrain Loss: 5.103 | Train PPL: 164.594\n",
      "\t Val. Loss: 6.037 |  Val. PPL: 418.588\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion)\n",
    "    valid_loss = evaluate(model, validation_dataloader, criterion)\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the loop to interact with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapting code from PyTorch chatbot tutorial: https://pytorch.org/tutorials/beginner/chatbot_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "        \n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        \n",
    "        #  Forward input through encoder model\n",
    "        #\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_batch)\n",
    "        encoder_outputs = encoder_outputs.squeeze(0)\n",
    "        print('encoder_outputs shape is', encoder_outputs.size())\n",
    "        \n",
    "        #  Prepare the encoder's final hidden layer to be first hidden input to the decoder\n",
    "        #\n",
    "        decoder_hidden = encoder_hidden.squeeze(0) # [:decoder.n_layers]\n",
    "        print('decoder_hidden shape is', decoder_hidden.size())\n",
    "        \n",
    "        #  Initialize decoder input with 0s, index of the SOS token\n",
    "        #\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * 0\n",
    "        print('decoder_input shape is', decoder_input.size())\n",
    "        \n",
    "        #  Initialize tensors to append decoded words to\n",
    "        #\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        \n",
    "        #  Iteratively decode one word token at a time\n",
    "        #\n",
    "        for _ in range(max_length):\n",
    "            \n",
    "            #  Forward pass through decoder\n",
    "            #\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            #  Obtain most likely word token and its softmax score\n",
    "            #\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            \n",
    "            #  Record token and score\n",
    "            #\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            \n",
    "            #  Prepare current token to be next decoder input (add a dimension)\n",
    "            #\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "            \n",
    "        #  Return collections of word tokens and scores\n",
    "        #\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, sentence, max_length=max_length):\n",
    "    \n",
    "    decoded_words = []\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        ### Format input sentence as a batch\n",
    "        #  words -> indexes\n",
    "        #\n",
    "        indexes_batch = [text_to_ids(sentence)]\n",
    "\n",
    "        #  Transpose dimensions of batch to match models' expectations\n",
    "        #\n",
    "        input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "\n",
    "        #  Use appropriate device\n",
    "        #\n",
    "        input_batch = input_batch.to(device)\n",
    "\n",
    "        #  Decode sentence with searcher\n",
    "        #\n",
    "        tokens, scores = searcher(input_batch)\n",
    "\n",
    "        #  indexes -> words\n",
    "        #\n",
    "        decoded_words = [id_to_word[token.item()] for token in tokens]\n",
    "    \n",
    "    except KeyError:\n",
    "            \n",
    "            print(\"Error: Encountered unknown word.\")\n",
    "            \n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateInput(encoder, decoder, searcher):\n",
    "    \n",
    "    input_sentence = ''\n",
    "    \n",
    "    while(1):\n",
    "        \n",
    "        #  Get input sentence\n",
    "        #\n",
    "        input_sentence = input('> ')\n",
    "\n",
    "        #  Check if it is quit case\n",
    "        #\n",
    "        if input_sentence == 'goodbye':\n",
    "\n",
    "            print('OK, catch you later!')\n",
    "            break\n",
    "\n",
    "        #  Preprocess sentence and convert to a list of tokens\n",
    "        #\n",
    "        user_tokens = clean_token_list(input_sentence)\n",
    "\n",
    "        #  Evaluate sentence\n",
    "        #\n",
    "        output_words = evaluate(encoder, decoder, searcher, user_tokens)\n",
    "\n",
    "        #  Format and print response sentence\n",
    "        #\n",
    "        output_words[:] = [x for x in output_words if not (x == '<eos>' or x == '<pad>')]\n",
    "\n",
    "        print('Bot:', ' '.join(output_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the saved model from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best-model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat loop; type 'goodbye' to end the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> hello\n",
      "encoder_outputs shape is torch.Size([1, 1024])\n",
      "decoder_hidden shape is torch.Size([1, 1024])\n",
      "decoder_input shape is torch.Size([1, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input must have 3 dimensions, got 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-de9d54f821ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#  Begin chatting (uncomment and run the following line to begin)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mevaluateInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-99-97ab4172c01c>\u001b[0m in \u001b[0;36mevaluateInput\u001b[0;34m(encoder, decoder, searcher)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#  Evaluate sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0moutput_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#  Format and print response sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-98-cf34f679b29d>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(encoder, decoder, searcher, sentence, max_length)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#  Decode sentence with searcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#  indexes -> words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-113264ec1e31>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_batch)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m#  Forward pass through decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m#  Obtain most likely word token and its softmax score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-52238bb99643>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source, hidden, cell)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    682\u001b[0m                            \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m                            ):\n\u001b[0;32m--> 684\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[1;32m    686\u001b[0m                                'Expected hidden[0] size {}, got {}')\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    201\u001b[0m             raise RuntimeError(\n\u001b[1;32m    202\u001b[0m                 'input must have {} dimensions, got {}'.format(\n\u001b[0;32m--> 203\u001b[0;31m                     expected_input_dim, input.dim()))\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             raise RuntimeError(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input must have 3 dimensions, got 5"
     ]
    }
   ],
   "source": [
    "#  Set encoder and decoder to eval mode\n",
    "#\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "#  Initialize search module\n",
    "#\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "#  Begin chatting\n",
    "#\n",
    "evaluateInput(encoder, decoder, searcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
